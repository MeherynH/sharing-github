{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c581aaed",
   "metadata": {},
   "source": [
    "# Student Performance Prediction\n",
    "## Part 1: Technical Implementation\n",
    "\n",
    "### Name: Meheryn Hossain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c17f6e4",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Machine Learning Task\n",
    "Suppose you work in the Advising Team for a large Portuguese school system, and your school director has asked you to analyze student data and create a machine learning model to predict a student’s performance based on select features. Your director hopes to use this information to identify students who might need additional assistance and interventions to improve their grades.\n",
    "\n",
    "Your task is to create a regression model to predict a student's grade. You will need to clean and prepare the data to ensure it is suitable for analysis. After building the model, you will evaluate its performance using appropriate metrics to assess its accuracy and effectiveness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba3422",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "\n",
    "Begin by importing and inspecting your dataset to ensure it is correctly loaded and understand its structure. This initial step sets the foundation for your analysis and modeling.\n",
    "\n",
    "1) **Import the Data**: Correctly import your data.\n",
    "2) **Initial Data Check**: Check the initial data, including size and data types.\n",
    "3) **Identify the Target**: Identify the target attribute.\n",
    "4) **Split the Data**: Split your data into training and test sets using the variable names `X_train`, `X_test`, `y_train`, and `y_test`.  Use `test_size=0.2` and `random_state=42`.\n",
    "5) **Comment Your Code**: Get into the habit of including comments in your code. Comments should explain <u>why</u> decisions were made, while the code should be clean enough to read and understand <u>what</u> the program does. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836b01f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#1. Import the Data\n",
    "students = pd.read_csv('student-mat.csv')\n",
    "#2. Initial Data Check\n",
    "students.info()\n",
    "#3. Identify the Target Attribute: G3 -> Final Year Grades because its the final outcome we're trying to predict.\n",
    "## G1 and G2 are grades from previous terms --> can be used as features/predictors in the model\n",
    " \n",
    "#4. Split the Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = students.drop(['G3'], axis=1)    ##features: droping target attribute/column from features(X)\n",
    "y = students['G3']                   ##target using G3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7396f0",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Understanding your data is a crucial step before building any machine learning model. This exploration phase helps you identify patterns, detect anomalies, and uncover insights that will guide your modeling decisions. By thoroughly analyzing and visualizing the data, you can make informed choices on feature selection and preprocessing, ultimately improving your model's performance and reliability.\n",
    "\n",
    "This section won't be automatically graded, but you must include your analytical insight and screenshots of your plots in the Executive Summary report.\n",
    "\n",
    "In this section you should:\n",
    "1) **Study Attributes**: Thoroughly study the training set attributes and their characteristics.\n",
    "2) **Visualizations**: Use visualizations to effectively analyze and explore your data. Be ready to explain what the visualization shows and why it is important.  \n",
    "3) **Correlations**: Analyze correlations between your numeric attributes.\n",
    "\n",
    "\n",
    "Include your analysis and at least three plots in your Executive Summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548143b1-95b4-40cc-80b8-a9989e799e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. study attributes: review student instructions document\n",
    "\n",
    "#2. Visualizations: (see required libraries below)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48b783-0578-4558-a437-d2e958842322",
   "metadata": {},
   "outputs": [],
   "source": [
    "##use correlation to find out which numerical features are most related to Final Grade G3  (Heatmap)\n",
    "numerical_columns = ['age', 'failures','absences_G1', 'absences_G2', 'absences_G3', 'G1', 'G2', 'G3', 'studytime']\n",
    "corr_matrix = students[numerical_columns].corr()\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "plt.title(\"Corr Matrix of Numerical Features\")\n",
    "plt.show()\n",
    "##output: strong predictors of G3 based on plot are --> G2, G1, age, studytime, absences_G1, absences_G2, absences_G3\n",
    "##ordinal: Medu, goout, Walc, traveltime, freetime, health\n",
    "##categorical: higher, romantic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191d48e-252b-4650-8af0-bdefab65fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution of Final Grades (Histogram): See how G3 is distributed throughout the dataset. Reasons = see if most students are passing/failing and Skewness in the grade distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(students['G3'], bins=15, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Distribution of Final Grades (G3)\")\n",
    "plt.xlabel(\"Final Grade\")\n",
    "plt.ylabel(\"Number of Students\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f884c-932d-4679-809d-dc223647fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numerical (Scatter): Relationship between Study Time and Final Grade: See how study time affects G3. Reasons = see if more study time correlates with higher grades\n",
    "students.plot(kind='scatter', x='studytime', y='G3', alpha=0.1, color='red') \n",
    "\n",
    "##Numerical (Box plot) - First Term Grade (G1) vs Final Grade (G3)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x='G1', y='G3', data=students)\n",
    "plt.title(\"First Term Grade (G1) vs Final Grade (G3)\")\n",
    "plt.xlabel(\"First Term Grade G1 (from 0 to 20)\")\n",
    "plt.ylabel(\"Final Grade G3 (from 0 to 20)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "##Numerical (Box plot) - Impact Past Failures vs Final Grade (G3)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x='failures', y='G3', data=students)\n",
    "plt.title(\"Impact of Past Failures on Final Grade\")\n",
    "plt.xlabel(\"Number of Past Class Failures (n if 1<=n<3, else 4)\")\n",
    "plt.ylabel(\"Final Grade G3 (from 0 to 20)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3669a38-6080-4934-aa64-3f6bcf109633",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Categorical (bar graph)- Impact of Internet on Final Grades\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(students.internet, students.G3, color='skyblue')\n",
    "plt.title(\"Impact of Internet Final Grade\")\n",
    "plt.xlabel(\"Impact of Internet\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.show()\n",
    "\n",
    "##Categorical (box plot)- Relationship between Parent's Cohabitation Status and Final Grade (G3). Reasons = see if stability plays a role in grade performance\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.boxplot(x='Pstatus', y='G3', data=students)\n",
    "plt.title(\"Parent's Cohabitation Status vs Final Grade\")\n",
    "plt.xlabel(\"Parent's Cohabitation Status (T = Together, A = Apart)\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "##Categorical (box plot) - Family Education Support Impact on Final Grades\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x='famsup', y='G3', data=students)\n",
    "plt.title(\"Family Education Support Impact on Final Grades\")\n",
    "plt.xlabel(\"Family Education Support\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.show()\n",
    "\n",
    "##Categorical (bar graph)- Higher Education Relationship to Final Grades\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(students.higher, students.G3, color='skyblue')\n",
    "plt.title(\"Higher Education Motivation on Final Grades\")\n",
    "plt.xlabel(\"Pursue Higher Education\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.show()\n",
    "\n",
    "##Categorical (bar graph)- Romantic Relationship with Grade Performance: See if romantic relationships impact academic results. Reasons = see of relationships have any negative affect on academics  and possibility of social support.\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(students.romantic, students.G3, color='skyblue')\n",
    "plt.title(\"Impact Romantic Relationships on Final Grade\")\n",
    "plt.xlabel(\"In Romantic Relationship\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68fd51d-47ee-42f1-805c-5200d3dc1ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ordinal (bar graph)- Mother's Level of Education on Final Grades\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(students.Medu, students.G3)\n",
    "plt.title(\"Mother's Level of Education on Final Grades\")\n",
    "plt.xlabel(\"Mother's Level of Education (0 = none, 1 = primary education, 2 = 5th to 9th grade, 3 = secondary education, 4 = higher education)\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.show()\n",
    "\n",
    "##Ordinal (box plot) - Father's Level of Education on Final Grades\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.boxplot(x='Fedu', y='G3', data=students)\n",
    "plt.title(\"Father's Level of Education on Final Grades\")\n",
    "plt.xlabel(\"Father's Level of Education (0 = none, 1 = primary education, 2 = 5th to 9th grade, 3 = secondary education, 4 = higher education)\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "##Ordinal (box plot) - Going out with Friends Impact on Final Grades\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.boxplot(x='goout', y='G3', data=students)\n",
    "plt.title(\"Going out with Friends Impact on Final Grades\")\n",
    "plt.xlabel(\"Going out with Friends (1 = very low to 5 = very high\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "##Ordinal (bar graph)- Impact of Workday Alcohol Consumption on Final Grades\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(students.Dalc, students.G3)\n",
    "plt.title(\"Impact of Workday Alcohol Consumption on Final Grades\")\n",
    "plt.xlabel(\"Workday Alcohol Consumption (1 = very low to 5 = very high\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.show()\n",
    "\n",
    "##Ordinal (box plot)- Impact of Weekend Alcohol Consumption on Final Grades\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.boxplot(x='Walc', y='G3', data=students)\n",
    "plt.title(\"Impact of Weekend Alcohol Consumption on Final Grades\")\n",
    "plt.xlabel(\"Workday Alcohol Consumption (1 = very low to 5 = very high\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "##Ordinal (box plot)- Impact of Travel Time on Final Grades\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.boxplot(x='traveltime', y='G3', data=students)\n",
    "plt.title(\"Impact of Travel Time on Final Grades\")\n",
    "plt.xlabel(\"Travel Time to School (1 = <15 min, 2 = 15 to 30 min, 3 = 30 min to 1 hour, 4 = >1 hour)\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "##Ordinal (box plot)- Impact of Family Relationships on Final Grades \n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.boxplot(x='famrel', y='G3', data=students)\n",
    "plt.title(\"Quality Family Relationships Impact on Final Grades\")\n",
    "plt.xlabel(\"Quality of Family Relationships (1 = very bad to 5 = excellent)\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "##Ordinal (box plot) - Impact of Free Time After School on Final Grades \n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.boxplot(x='freetime', y='G3', data=students)\n",
    "plt.title(\"Impact of Free Time After School on Final Grades\")\n",
    "plt.xlabel(\"Free Time After School (1 = very low to 5 = very high)\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "##Ordinal (box plot)- Impact of Health on Final Grades \n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.boxplot(x='health', y='G3', data=students)\n",
    "plt.title(\"Impact of Health on Final Grades\")\n",
    "plt.xlabel(\"Current Health Status (1 = very bad to 5 = very good)\")\n",
    "plt.ylabel(\"Final Grade (G3)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9425c-79bd-4607-bea7-fc41ec709b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    " ##create scatterplot     students.plot(kind='scatter', x='failures', y='G3', alpha=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520251cb",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "Based on your data exploration, begin considering the features you want to include in your model. Limiting your data can be beneficial because it reduces complexity and can improve model performance by focusing on the most relevant features.\n",
    "\n",
    "Create lists below for the columns you want to use in your model based on your exploration above. These features will be used in the column transformer. The list names must match exactly.\n",
    "\n",
    "- **numeric_columns**: This is your continuous numerical data that MUST include `absences_G1`, `absences_G2`, `absences_G3`, `G1`, and `G2` for use in your custom transformer, in addition to any other numerical columns you want to select. Note: The fact that a column is labeled as an integer or float does not necessarily indicate that it contains continuous data.\n",
    "- **categorical_columns**: Include at least one categorical column.\n",
    "- **ordinal_columns**: Include at least one ordinal column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70809f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create list for numeric_columns:\n",
    "numeric_columns = ['age', 'studytime', 'absences_G1', 'absences_G2', 'absences_G3', 'G1', 'G2', 'failures']\n",
    "\n",
    "##Create list for categorical_columns:\n",
    "categorical_columns = ['higher', 'internet', 'romantic']\n",
    "\n",
    "##Create list for ordinal_columns:\n",
    "ordinal_columns = ['Medu', 'goout', 'Walc', 'traveltime', 'freetime', 'health']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4404e6d9",
   "metadata": {},
   "source": [
    "### Custom Transformer\n",
    "We want to create a new column that sums the three absences columns together as a new feature. Additionally, we want to  conditionally keep or drop the grades for the first and second terms based on the parameters passed.\n",
    "\n",
    "G3 is the final year grade and is highly correlated with G2 and G1, which are grades from the first two terms. Predicting G3 without using G2 and G1 is more challenging but also more valuable since you could make predictions earlier in the year. Therefore, later we will create separate models (one that includes the G1 and G2 columns and one that excludes them) to test this.\n",
    "\n",
    "#### Instructions for Submission\n",
    "\n",
    "Create a custom transformer that:\n",
    "\n",
    "- Inherits from BaseEstimator and TransformerMixin.\n",
    "- Implements the fit and transform methods.\n",
    "- Accepts a DataFrame as input. This differs from the California Housing Prices example, which used arrays. We will pass a DataFrame into the custom transformer to allow for easier testing with CodeGrade.\n",
    "- In the transform method:\n",
    "    - Create a new column called `absences_sum` that sums the `absences_G1`, `absences_G2`, and `absences_G3` columns, adds the new `absences_sum` column to the end of the DataFrame, then drops the original three absence columns.\n",
    "    - Drop the `G1` and `G2` columns if the parameter `drop_grades` is `True`. It will keep the columns if `drop_grades` is `False`.\n",
    "- Name the custom transformer class `FinalProjectTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fdd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create a custom transformer:\n",
    "##inherits from BaseEstimator and TransformerMixin\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "##column index:\n",
    "#absences_G1_ix, absences_G2_ix, absences_G3_ix, G1_ix, G2_ix = 29, 30, 31, 32, 33\n",
    "\n",
    "##name the custom transfomer FinalProjectTransformer and in it:\n",
    "##create new column called absences_sum --> sum up absences_G1, absences_G2, and absences_G3 columns,\n",
    "##then add new absences_sum column to end of DataFrame then drop the 3 original absence columns. \n",
    "##if drop_grades= True, then drop G1 abd G2 columns.\n",
    "##if drop_grades = False, then keep the columns G1 and G2.\n",
    "##fit and transform data\n",
    "\n",
    "class FinalProjectTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_grades=True):    ##initializes the drop_grades attributes\n",
    "        self.drop_grades = drop_grades\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X['absences_sum'] = ( X['absences_G1'] + X['absences_G2'] + X['absences_G3'])  ##creates absences_sum column\n",
    "        X = X.drop(['absences_G1', 'absences_G2', 'absences_G3'], axis=1)      ##drops the 3 original absence columns\n",
    "    \n",
    "        if self.drop_grades:   ##if drop_grades = True, then drop G1 abd G2 columns\n",
    "            X = X.drop(['G1', 'G2'], axis=1)\n",
    "            return X\n",
    "\n",
    "        else:\n",
    "            return X     ##else is for when if drop_grades = True is skipped\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c8fb5",
   "metadata": {},
   "source": [
    "### Data Pipelines Instructions\n",
    "Creating data pipelines allows you to automate your data cleaning process, making it easy to apply the same transformations to new data. Follow the outline below to transform your dataset into two sets of transformed data: one with the G1/G2 columns and one without them.\n",
    "\n",
    "#### Instructions for Submission\n",
    "- Numeric Pipeline (you'll need to create two to handle the G1/G2 requirement)\n",
    "  - Impute missing values using SimpleImputer() (use [.set_output(transform=\"pandas\")](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_set_output.html) to output a DataFrame from your SimpleImputer into your custom transformer) \n",
    "  - Transform data using the custom transformer FinalProjectTransformer as appropriate for the task\n",
    "  - Standardize the data using StandardScalar()\n",
    "  - Use the following variable names:\n",
    "    - `numeric_pipeline_with_grades`\n",
    "    - `numeric_pipeline_without_grades`\n",
    "\n",
    "- Categorical Pipeline\n",
    "  - Impute missing values \n",
    "  - One-Hot Encode (OHE) categorical data \n",
    "  - Use the following variable name:\n",
    "    - `categorical_pipeline`\n",
    "\n",
    "- Ordinal Pipeline\n",
    "  - Impute missing values \n",
    "  - Ordinal encode the data\n",
    "  - Use the following variable name:\n",
    "    - `ordinal_pipeline`\n",
    "\n",
    "- Column Transformer (you'll need to create two to handle the two different numeric pipelines)\n",
    "  - pass in your previously created feature selection lists\n",
    "  - Combine the numeric, categorical, and ordinal pipelines\n",
    "  - Use the following variable names:\n",
    "    - `column_transformer_with_grades`\n",
    "    - `column_transformer_without_grades`\n",
    "    \n",
    "Once the full pipeline is set up, fit and transform `X_train`, saving the results as `X_train_transformed_with_grades` and `X_train_transformed_without_grades`. Confirm that the transformed data without grades has two fewer columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92892c8f-9302-4031-b69a-430b4217dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports for Numeric\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## Numeric Pipeline: numeric_pipeline_with_grades AND numeric_pipeline_without_grades\n",
    "numeric_pipeline_with_grades = Pipeline([\n",
    "    ('Imputer', SimpleImputer().set_output(transform=\"pandas\")),    ##Imputing using SimpleImputer\n",
    "    ('custom_transformer', FinalProjectTransformer(drop_grades=False)), ##drop_grades=False, keeps G1, G2\n",
    "    ('std_scaler', StandardScaler())])                                ##Feature, scaling with StandardScaler\n",
    "\n",
    "numeric_pipeline_without_grades = Pipeline([\n",
    "    ('Imputer', SimpleImputer().set_output(transform=\"pandas\")),\n",
    "    ('custom_transformer', FinalProjectTransformer()),  ##drop_grades=True so it drops G1, G2\n",
    "    ('std_scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29ec1a-8c87-451c-ba3a-a8fa3e5b2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorical Pipeline: imports below for One-Hot Encode (OHE), categorical_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('Imputer', SimpleImputer(strategy='most_frequent')),   ##use SimpleImputer to impute missing values\n",
    "    ('cat_encoder', OneHotEncoder())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fdb72-5840-4065-89f0-d9a86032f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ordinal Pipeline: import Ordinal Encoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_pipeline = Pipeline([\n",
    "    ('Imputer', SimpleImputer(strategy='most_frequent')), ##use SimpleImputer to impute missing values\n",
    "    ('ordinal', OrdinalEncoder())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d33fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ColumnTransformer: column_transformer_with_grades AND column_transformer_without_grades\n",
    "from sklearn.compose import ColumnTransformer    ##Column Transformer splits apart training data into numeric, categorical and ordinal values\n",
    "\n",
    "##1. create two ColumnTransformer: \n",
    "\n",
    "##2. create ColumnTransformer with grades: column_transformer_with_grades\n",
    "column_transformer_with_grades = ColumnTransformer([\n",
    "    (\"num\", numeric_pipeline_with_grades, numeric_columns),  ##ColumnTransformer to work with Numeric Pipeline: numeric_pipeline_with_grades\n",
    "    (\"cat\", categorical_pipeline, categorical_columns),\n",
    "    (\"ord\", ordinal_pipeline, ordinal_columns)])\n",
    "\n",
    "##3. create ColumnTransformer without grades: column_transformer_without_grades\n",
    "column_transformer_without_grades = ColumnTransformer([\n",
    "    (\"num\", numeric_pipeline_without_grades, numeric_columns),  ##ColumnTransformer to work with Numeric Pipeline: numeric_pipeline_without_grades\n",
    "    (\"cat\", categorical_pipeline, categorical_columns),\n",
    "    (\"ord\", ordinal_pipeline, ordinal_columns)])\n",
    "\n",
    "##fit and transform X_train, save as X_train_transformed_with_grades AND X_train_transformed_without_grades. \n",
    "X_train_transformed_with_grades = column_transformer_with_grades.fit_transform(X_train)\n",
    "X_train_transformed_without_grades = column_transformer_without_grades.fit_transform(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c61b62-b3a7-44df-8102-6b8790ba307c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Confirm that the transformed data without grades has two fewer columns. print number of columns use length()[0]\n",
    "print(\"Number of columns in X_train_transformed_with_grades:\", len(X_train_transformed_with_grades[0]))\n",
    "print(\"Number of columns in X_train_transformed_without_grades:\", len(X_train_transformed_without_grades[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bacc10",
   "metadata": {},
   "source": [
    "## Shortlist Promising Models\n",
    "In this section, you will fit and compare three regression models to your transformed data, both with and without the G1/G2 columns, using cross-validation. Follow the steps below, using the specified variable names.\n",
    "\n",
    "1) **Initialize Three Regression Models**\n",
    "- Linear Regression\n",
    "- Support Vector Machine (SVM) Regression\n",
    "- Lasso Regression\n",
    "\n",
    "2) **Compare Models with Cross-Validation**\n",
    "- Using the above models, perform cross-validation on each model using both sets of transformed data (with and without G1/G2 columns).\n",
    "\n",
    "### Instructions for Submission\n",
    "1) **Initialize the Models**: Instantiate a Linear Regression, SVM Regression, and Lasso Regression model.\n",
    "  - Use the specified variable names for the respective models:\n",
    "    - `lin_reg`\n",
    "    - `svm_reg`\n",
    "    - `lasso_reg`\n",
    "2) **Cross-Validation**: Using both sets of transformed data (with and without G1/G2 columns), perform 3-fold cross-validation for each model using RMSE as the metric.\n",
    "  - You will run cross-validation six times (e.g., cross-validation of the linear regression model with the G1/G2 data, cross-validation of the linear regression model without the G1/G2 data, etc.)\n",
    "  - Use the specified variable names to save each respective array of scores:\n",
    "    - `cv_scores_lin_reg_with_grades`\n",
    "    - `cv_scores_lin_reg_without_grades`\n",
    "    - `cv_scores_svm_with_grades`\n",
    "    - `cv_scores_svm_without_grades`\n",
    "    - `cv_scores_lasso_with_grades`\n",
    "    - `cv_scores_lasso_without_grades`\n",
    "  - Use the specified variable names to save the mean of each cross-validation array and print it to view your mean scores:\n",
    "    - `rmse_lin_reg_with_grades`\n",
    "    - `rmse_lin_reg_without_grades`\n",
    "    - `rmse_svm_with_grades`\n",
    "    - `rmse_svm_without_grades`\n",
    "    - `rmse_lasso_with_grades`\n",
    "    - `rmse_lasso_without_grades`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de8cc1-54ad-4a16-ad77-9a1313c21e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import and initialize the regression models\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.svm import SVR\n",
    "lin_reg = LinearRegression()\n",
    "svm_reg = SVR()\n",
    "lasso_reg = Lasso()\n",
    "\n",
    "##Run cross validation 6 times: With and Without G1/G2 data for all 3 regression models AND 3-fold, RMSE scoring **use transformed data**\n",
    "from sklearn.model_selection import cross_val_score\n",
    "##1. LinearRegression\n",
    "cv_scores_lin_reg_with_grades = np.sqrt(-cross_val_score(lin_reg, X_train_transformed_with_grades, y_train, \n",
    "                                                         scoring= 'neg_mean_squared_error', cv=3))\n",
    "\n",
    "cv_scores_lin_reg_without_grades = np.sqrt(-cross_val_score(lin_reg, X_train_transformed_without_grades, y_train, \n",
    "                                                            scoring= 'neg_mean_squared_error', cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e380c-0ce2-4143-9000-b86d081b872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2. SVM Regression\n",
    "cv_scores_svm_with_grades = np.sqrt(-cross_val_score(svm_reg, X_train_transformed_with_grades, y_train, \n",
    "                                                         scoring= 'neg_mean_squared_error', cv=3))\n",
    "\n",
    "cv_scores_svm_without_grades = np.sqrt(-cross_val_score(svm_reg, X_train_transformed_without_grades, y_train, \n",
    "                                                            scoring= 'neg_mean_squared_error', cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3383d45-efca-4dd8-a53b-f13fb4bb95a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##3. Lasso Regression\n",
    "cv_scores_lasso_with_grades = np.sqrt(-cross_val_score(lasso_reg, X_train_transformed_with_grades, y_train, \n",
    "                                                         scoring= 'neg_mean_squared_error', cv=3))\n",
    "\n",
    "cv_scores_lasso_without_grades = np.sqrt(-cross_val_score(lasso_reg, X_train_transformed_without_grades, y_train, \n",
    "                                                         scoring= 'neg_mean_squared_error', cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b6248",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save the RMSE mean of each cross-validation array and print \n",
    "rmse_lin_reg_with_grades = cv_scores_lin_reg_with_grades.mean()\n",
    "rmse_lin_reg_without_grades = cv_scores_lin_reg_without_grades.mean()\n",
    "rmse_svm_with_grades = cv_scores_svm_with_grades.mean()\n",
    "rmse_svm_without_grades = cv_scores_svm_without_grades.mean()\n",
    "rmse_lasso_with_grades = cv_scores_lasso_with_grades.mean()\n",
    "rmse_lasso_without_grades = cv_scores_lasso_without_grades.mean()\n",
    "\n",
    "##print\n",
    "print(rmse_lin_reg_with_grades)\n",
    "print(rmse_lin_reg_without_grades)\n",
    "print(rmse_svm_with_grades)\n",
    "print(rmse_svm_without_grades)\n",
    "print(rmse_lasso_with_grades)\n",
    "print(rmse_lasso_without_grades)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8665ea9",
   "metadata": {},
   "source": [
    "## Fine-Tune the System\n",
    "In this section, you will use the Support Vector Machine (SVM) regression model and perform grid search to fine-tune its hyperparameters. Follow the steps below to set up the grid search, ensuring you use the specified variable names for automatic grading through CodeGrade.\n",
    "\n",
    "1) Set Up Grid Search for SVM Regression\n",
    "  - Define a parameter grid to search over. Review Scikit-learn's documentation for the available hyperparameters for this algorithm.\n",
    "  - Use GridSearchCV to find the best hyperparameters.\n",
    "  - Fit the grid search to both sets (with and without the G1/G2 columns) of the transformed training data.\n",
    "\n",
    "### Instructions for Submission\n",
    "\n",
    "1) **Define Parameter Grid**: Set up a parameter grid for the SVM regression model name `param_grid`.\n",
    "2) **Initialize Grid Search**: Initialize the `GridSearchCV` and call this `grid_search`.\n",
    "3) **Fit the Grid Search**: Fit the grid search to both sets (with and without the G1/G2 columns) of the transformed training data.\n",
    "4) **Save & Print Best Parameters**: Save the best parameters for each respective fit to `best_params_with_grades` and `best_params_without_grades`, and print them.\n",
    "5) **Print Best Score**: Use the `best_score_` attribute to view the mean cross-validated score for each respective best_estimator.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a77cdf-ce44-48b5-b50d-6b8d307c89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import LinearSVR and GridSearchCV, resource for SVM regression model: support_vector_machines notebook - 5. SVM Regression\n",
    "#from sklearn.svm import LinearSVR      \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2501d9-92f5-4c7b-aa7b-66e9afeb61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##1.Define Parameter Grid name it param_grid. Make sure to use correct Hyperparameter with GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100],\n",
    "              'gamma': [1, 0.1, 0.01, 0.001],\n",
    "              'kernel':['rbf']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a417153-94f4-41f3-b4aa-4eea5922768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create pipeline for SVM regression \n",
    "\n",
    "#svm_reg = Pipeline([\n",
    "#    ('scaler', StandardScaler()), \n",
    "#    ('svm', LinearSVR(epsilon=0.5, dual='auto', random_state=42))]) ##the pipeline we will run it through a StandardScaler, LinearSVR with epsilon and dual(to prevent warning message) and random state of 42.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b6fb6d-a40d-45c1-874c-7528c678e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2. Initialize GridSearchCV and name is grid_search\n",
    "grid_search = GridSearchCV(SVC(), param_grid, verbose=1, cv=3, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4571d6d1-5d3a-4d80-914c-ad64b6fd25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##3. Fit grid search with G1 and G2 using transformed training data. best_params_with_grades AND best_params_without_grad\n",
    "grid_search.fit(X_train_transformed_with_grades, y_train)\n",
    "best_params_with_grades = grid_search.best_params_\n",
    "print(best_params_with_grades)\n",
    "\n",
    "best_score_with_grades = np.sqrt(-grid_search.best_score_)\n",
    "print(best_score_with_grades)\n",
    "\n",
    "Best_model_with_grades = grid_search.best_estimator_\n",
    "print(Best_model_with_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##4. Fit grid search WITHOUT G1 and G2 using transformed training data.: best_params_with_grades AND best_params_without_grades\n",
    "grid_search.fit(X_train_transformed_without_grades, y_train)\n",
    "best_params_without_grades = grid_search.best_params_\n",
    "print(best_params_without_grades)\n",
    "\n",
    "best_score_without_grades = np.sqrt(-grid_search.best_score_)\n",
    "print(best_score_without_grades)\n",
    "\n",
    "Best_model_without_grades = grid_search.best_estimator_\n",
    "print(Best_model_without_grades)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07083c80",
   "metadata": {},
   "source": [
    "## Measure Performance on Test Set\n",
    "In this section, you will transform the test set using your full pipeline and measure the performance of your best model on the test set. \n",
    "\n",
    "1) Based on all previous cross-validation results, pick your best model.\n",
    "2) Use the previously created column transformers to transform the test set, both with and without the G1/G2 columns.\n",
    "3) Using your best model, measure its performance on the test set to estimate the generalization error.\n",
    "  \n",
    "### Instructions for Submission\n",
    "1) **Fit Best Model**: If you haven't already, fit your best model to both sets of your transformed training data. \n",
    "2) **Transform the Test Set**: Use your column transformers to transform the test set (`X_test`), both with and without the G1/G2 columns. Name these transformed datasets `X_test_transformed_with_grades` and `X_test_transformed_without_grades`.\n",
    "3) **Evaluate Performance**: Measure the performance of your best-fitted models on the transformed test sets using Root Mean Squared Error (RMSE) and R-squared (R²) metrics. Save these variables as:\n",
    "  - `rmse_with_grades`\n",
    "  - `r2_with_grades`\n",
    "  - `rmse_without_grades`\n",
    "  - `r2_without_grades`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a016ba8-9eee-4b01-806e-798a20260089",
   "metadata": {},
   "outputs": [],
   "source": [
    "##1. Best Model based on previous Cross Validation results: use .best_estimator\n",
    "print(Best_model_with_grades)\n",
    "print(Best_model_without_grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##2. Transform the Test Set: ColumnTransformer from previous code\n",
    "X_test_transformed_with_grades = column_transformer_with_grades.transform(X_test)   \n",
    "X_test_transformed_without_grades = column_transformer_without_grades.transform(X_test)\n",
    "\n",
    "##3. Evaluate Performance: \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_predictions_with_grades = Best_model_with_grades.predict(X_test_transformed_with_grades)\n",
    "rmse_with_grades = np.sqrt(mean_squared_error(y_test, y_predictions_with_grades))\n",
    "r2_with_grades = r2_score(y_test, y_predictions_with_grades)\n",
    "\n",
    "y_predictions_without_grades = Best_model_without_grades.predict(X_test_transformed_without_grades)\n",
    "rmse_without_grades = np.sqrt(mean_squared_error(y_test, y_predictions_without_grades))\n",
    "r2_without_grades = r2_score(y_test, y_predictions_without_grades)\n",
    "\n",
    "print(rmse_with_grades)\n",
    "print(r2_with_grades)\n",
    "print(rmse_without_grades)\n",
    "print(r2_without_grades)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bdc5a0",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "Once you complete all the steps above, you will:\n",
    "\n",
    "1) Upload your `final_project.ipynb` to the **Final Project Notebook Submission** link in Brightspace to check your work.\n",
    "2) Finalize your **Executive Summary** document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtsc670",
   "language": "python",
   "name": "dtsc670"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
